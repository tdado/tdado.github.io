---
title: "Brain2GAN; Reconstructing perceived faces from the primate brain via StyleGAN"
layout: archive
author_profile: true
classes: wide
---

## Abstract
Neural coding characterizes the relationship between stimuli and their corresponding neural responses. The usage of synthesized yet photorealistic reality by generative adversarial networks (GANs) allows for superior control over these data: the underlying feature representations that account for the semantics in synthesized data are known a priori and their relationship is perfect rather than approximated post-hoc by feature extraction models. We exploit this property in neural decoding of multi-unit activity responses that we recorded from the primate brain upon presentation with synthesized face images in a passive fixation experiment. The face reconstructions we acquired from brain activity were astonishingly similar to the originally perceived face stimuli. This provides strong evidence that the neural face manifold and the disentangled w-latent space conditioned on StyleGAN3 (rather than z-latent space of arbitrary GANs) share how they represent the high-level semantics of the high-dimensional space of faces.

Full paper: TBA.

![b2g_image](/assets/images/b2g_image.png)
*The 100 test set stimuli and their reconstructions from brain activity.*